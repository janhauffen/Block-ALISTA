{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "stability Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JGJzwfspiTK8",
        "dXzz17UViVjc",
        "mhBgZFZHiYBh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eDRtPEOfP-M"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# %tensorflow_version 1.14\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy.linalg as la\n",
        "import scipy.io as sio\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import cvxpy as cp\n",
        "import blocksparsetoolbox as bst"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGJzwfspiTK8"
      },
      "source": [
        "# Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwDulE-lg0zI"
      },
      "source": [
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, A, **kwargs):\n",
        "        self.A = A\n",
        "        M, N = A.shape\n",
        "        vars(self).update(kwargs)\n",
        "        self.x_ = tf.placeholder(tf.float32, (N, None), name='x')\n",
        "        self.y_ = tf.placeholder(tf.float32, (M, None), name='y')\n",
        "\n",
        "\n",
        "class TFGenerator(Generator):\n",
        "    def __init__(self, **kwargs):\n",
        "        Generator.__init__(self, **kwargs)\n",
        "\n",
        "    def __call__(self, sess):\n",
        "        \"generates y,x pair for training\"\n",
        "        return sess.run((self.ygen_, self.xgen_))\n",
        "\n",
        "\n",
        "def block_gaussian_trial(m=128, L=32, B=16, MC=1000, pnz=.1, SNR_dB=20):\n",
        "    N = B * L  # N is the length of a the unknown block-sparse x\n",
        "    A2 = np.random.normal(size=(m, N), scale=1.0 / math.sqrt(m)).astype(np.float32)\n",
        "    # pdb.set_trace()\n",
        "    A2 = sio.loadmat('small_normalized_D.mat')\n",
        "    A2 = A2.get('D')\n",
        "    A=A2.astype('float32')\n",
        "    A_ = tf.constant(A, name='A')\n",
        "    prob = TFGenerator(A=A, A_=A_, kappa=None, SNR=SNR_dB)\n",
        "    #pdb.set_trace()\n",
        "    # prob.A=A\n",
        "    prob.name = 'block sparse, Gaussian A'\n",
        "    prob.L = L\n",
        "    prob.B = B\n",
        "    prob.N = N\n",
        "    prob.m = m\n",
        "    # prob.SNR_dB = SNR_dB\n",
        "    prob.pnz = pnz\n",
        "\n",
        "    # Create tf vectors\n",
        "    active_blocks_ = tf.to_float(tf.random_uniform((L, 1 , MC)) < pnz)\n",
        "    ones_ = tf.ones([L, B, MC])\n",
        "\n",
        "    product_ = tf.multiply(active_blocks_, ones_)\n",
        "    xgen_ = tf.reshape(product_, [L * B, MC])\n",
        "    xgen_ = tf.multiply(xgen_, tf.random_normal((N, MC), 0, 1))\n",
        "\n",
        "    # you should probably change the way noise_var is calculated\n",
        "    noise_var = pnz * N / m * math.pow(10., -SNR_dB / 10.)\n",
        "    ygen_ = tf.matmul(A_, xgen_) + tf.random_normal((m, MC), stddev=math.sqrt(noise_var))\n",
        "    #ygen_ = tf.matmul(A_, xgen_)\n",
        "\n",
        "    active_blocks_val = (np.random.uniform(0, 1, (L, MC)) < pnz).astype(np.float32)\n",
        "    active_entries_val = np.repeat(active_blocks_val, B, axis=0)\n",
        "    xval = np.multiply(active_entries_val, np.random.normal(0, 1, (N, MC)))\n",
        "    yval = np.matmul(A, xval) + np.random.normal(0, math.sqrt(noise_var), (m, MC))\n",
        "    #yval = np.matmul(A,xval)\n",
        "\n",
        "    prob.xgen_ = xgen_\n",
        "    prob.ygen_ = ygen_\n",
        "    prob.xval = xval\n",
        "    prob.yval = yval\n",
        "    prob.noise_var = noise_var\n",
        "    prob.noise = np.random.normal(0, math.sqrt(noise_var), (m))\n",
        "\n",
        "    return prob\n",
        "\n",
        "def createsparse(N,n):\n",
        "    sig = np.zeros(N)\n",
        "    k = np.random.randint(0, N, n)\n",
        "    for a in range(n):\n",
        "        for j in range(N):\n",
        "            if j == k[a]:\n",
        "                sig[j] = 1\n",
        "    return sig\n",
        "\n",
        "def new_block_gaussian_trial(m=128, L=32, B=16, MC=1000, sparsity=2, SNR_dB=20):\n",
        "    N = B * L  # N is the length of a the unknown block-sparse x\n",
        "    pnz = 0.1\n",
        "    # A2 = np.random.normal(size=(m, N), scale=1.0 / math.sqrt(m)).astype(np.float32)\n",
        "    A2 = sio.loadmat('Dblocksparse.mat')\n",
        "    A2 = A2.get('D')\n",
        "    A=A2\n",
        "    A_ = tf.constant(A, name='A')\n",
        "    prob = TFGenerator(A=A, A_=A_, kappa=None, SNR=SNR_dB)\n",
        "    #pdb.set_trace()\n",
        "    # prob.A=A\n",
        "    prob.name = 'block sparse, Gaussian A'\n",
        "    prob.L = L\n",
        "    prob.B = B\n",
        "    prob.N = N\n",
        "    # prob.SNR_dB = SNR_dB\n",
        "    prob.pnz = pnz\n",
        "\n",
        "    # Create tf vectors\n",
        "\n",
        "    o = np.zeros((sparsity, sparsity))\n",
        "    o[:, 0] = np.random.randint(0, L, sparsity)\n",
        "    active_blocks_ = tf.to_float(tf.random_uniform((L, 1 , MC)) < pnz)\n",
        "    ones_ = tf.ones([L, B, MC])\n",
        "\n",
        "    product_ = tf.multiply(active_blocks_, ones_)\n",
        "    xgen_ = tf.reshape(product_, [L * B, MC])\n",
        "    xgen_ = tf.multiply(xgen_, tf.random_normal((N,MC), 0, 1 ))\n",
        "\n",
        "    # you should probably change the way noise_var is calculated\n",
        "    noise_var = pnz * N / m * math.pow(10., -SNR_dB / 10.)\n",
        "    ygen_ = tf.matmul(A_, xgen_) + tf.random_normal((m, MC), stddev=math.sqrt(noise_var))\n",
        "    #ygen_ = tf.matmul(A_, xgen_)\n",
        "\n",
        "    active_blocks_val = np.zeros((L,MC))\n",
        "    for i in range(MC):\n",
        "        active_blocks_val[:,i] = createsparse(L, sparsity)\n",
        "    #pdb.set_trace()\n",
        "    active_entries_val = np.repeat(active_blocks_val, B, axis=0)\n",
        "    xval = np.multiply(active_entries_val, np.random.normal(0, 1, (N, MC)))\n",
        "    yval = np.matmul(A, xval) + np.random.normal(0, math.sqrt(noise_var), (m, MC))\n",
        "    #yval = np.matmul(A,xval)\n",
        "\n",
        "    prob.xgen_ = xgen_\n",
        "    prob.ygen_ = ygen_\n",
        "    prob.xval = xval\n",
        "    prob.yval = yval\n",
        "    prob.noise_var = noise_var\n",
        "    prob.noise = np.random.normal(0, math.sqrt(noise_var), (m, MC))\n",
        "\n",
        "    return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXzz17UViVjc"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0JYUYoZhNjq"
      },
      "source": [
        "import cvxpy as cp\n",
        "\n",
        "def R(D, n, d): #function to compute the generalized coherence for D = B^TD\n",
        "    n_y, n_x = D.shape\n",
        "    R = np.zeros((n, n))\n",
        "    for k in range(0, n):\n",
        "        for l in range(k, n):\n",
        "            I = np.zeros((n_x, n_x))\n",
        "            I_s = np.zeros((n, n))\n",
        "            I[l * d:(l + 1) * d, k * d:(k + 1) * d] = np.ones(d)\n",
        "            I_s[l, k] = 1\n",
        "            if k == l:\n",
        "                R = R + cp.norm(cp.multiply(I, D), 2) * I_s\n",
        "            else:\n",
        "                R = R + cp.norm(cp.multiply(I, D), 2) * I_s + cp.norm(cp.multiply(I.T, D), 2) * I_s.T\n",
        "\n",
        "    return R\n",
        "\n",
        "def simple_soft_threshold(r_, lam_):\n",
        "    \"implement a soft threshold function y=sign(r)*max(0,abs(r)-lam)\"\n",
        "    lam_ = tf.maximum(lam_, 0)\n",
        "    #pdb.set_trace()\n",
        "    return tf.sign(r_) * tf.maximum(tf.abs(r_) - lam_, 0)\n",
        "\n",
        "def block_soft_threshold(X_, al_):\n",
        "    B = 15\n",
        "    L = 5\n",
        "    shape = K.shape(X_)\n",
        "    pool_shape1 = tf.stack([B, L, shape[1]])\n",
        "    al_ = tf.maximum(al_, 0)\n",
        "    Xnew_ = K.reshape(X_, pool_shape1)  # i'th coloumn of X_\n",
        "    r_ = tf.sqrt(tf.reduce_sum((Xnew_ ** 2), 1))\n",
        "    r_ = tf.maximum(.0, 1 - tf.math.divide_no_nan(al_ , r_))\n",
        "    shaper = K.shape(r_)\n",
        "    pool_r = tf.stack([B, 1, shaper[1]])\n",
        "    r_ = K.reshape(r_, pool_r)  # Diesen reshape Befehl auch zu K.reshape ändern\n",
        "    pool_shape2 = tf.stack([B * L, shape[1]])\n",
        "\n",
        "    Xnew_ = tf.multiply(Xnew_, r_)\n",
        "\n",
        "    return K.reshape(Xnew_, pool_shape2)  # transpose, bc of the list and tf.stack() operator\n",
        "\n",
        "def block_soft_threshold_elastic_net(X_, al_, la_):\n",
        "    B = 64\n",
        "    L = 1\n",
        "\n",
        "    shape = K.shape(X_)\n",
        "    pool_shape1 = tf.stack([B, L, shape[1]])\n",
        "    al_ = tf.maximum(al_, 0)\n",
        "    Xnew_ = K.reshape(X_, pool_shape1)  # i'th coloumn of X_\n",
        "    r_ = tf.sqrt(tf.reduce_sum((Xnew_ ** 2), 1))\n",
        "    r_ = tf.maximum(.0, 1 - tf.math.divide_no_nan(al_ , r_))\n",
        "    shaper = K.shape(r_)\n",
        "    pool_r = tf.stack([B, 1, shaper[1]])\n",
        "    r_ = K.reshape(r_, pool_r)  # Diesen reshape Befehl auch zu K.reshape ändern\n",
        "    pool_shape2 = tf.stack([B * L, shape[1]])\n",
        "\n",
        "    Xnew_ = tf.multiply(Xnew_, r_)*(1+la_)**(-1)\n",
        "\n",
        "    return K.reshape(Xnew_, pool_shape2)  # transpose, bc of the list and tf.stack() operator\n",
        "\n",
        "def build_LBISTA(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft=block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    By_ = tf.matmul(B_,prob.y_)\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    #pdb.set_trace()\n",
        "    #layers.append( ('Linear',By_,None) )\n",
        "    \n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    #if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = blocksoft( By_, lam0_)\n",
        "    layers.append( ('LBISTA T=1',xhat_, (lam0_,B_, S_) ) )\n",
        "    #pdb.set_trace()\n",
        "    for t in range(1,T):\n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        xhat_ = blocksoft( tf.matmul(S_,xhat_) + tf.matmul(B_,prob.y_), lam_ )\n",
        "        layers.append( ('LBISTA T='+str(t+1),xhat_,(lam_,B_, S_)) )\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "\n",
        "def build_LBISTA_untied(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft=block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    By_ = tf.matmul(B_,prob.y_)\n",
        "    #pdb.set_trace()\n",
        "    #layers.append( ('Linear',By_,None) )\n",
        "    \n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    #if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = blocksoft( By_, lam0_)\n",
        "    layers.append( ('LBISTA T=1',xhat_, (lam0_,B_, S_) ) )\n",
        "    #pdb.set_trace()\n",
        "    for t in range(1,T):\n",
        "        B_ =  tf.Variable(B,dtype=tf.float32,name='B_{0}'.format(t) )\n",
        "        S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_{0}'.format(t) )\n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        xhat_ = blocksoft( tf.matmul(S_,xhat_) + tf.matmul(B_,prob.y_), lam_ )\n",
        "        layers.append( ('LBISTA T='+str(t+1),xhat_,(lam_,B_, S_)) )\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "def build_ALISTA(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = simple_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    Wmat = sio.loadmat('W.mat') #W has to be precomputed before running the script\n",
        "    W = Wmat.get('W')\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    W_ = tf.Variable(np.transpose(W), dtype=tf.float32, trainable = False)\n",
        "    layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "def build_TiBLISTA(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    #Wmat = sio.loadmat('W.mat') #W has to be precomputed before running the script\n",
        "    W = A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    W_ = tf.Variable(np.transpose(W), dtype=tf.float32, trainable = True)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,gamma0_,W_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_,W_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "\n",
        "def build_LBISTA_CPSS(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    #Wmat = sio.loadmat('W.mat') #W has to be precomputed before running the script\n",
        "    W = A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    W_ = tf.Variable(np.transpose(W), dtype=tf.float32, trainable = True)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,gamma0_,W_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        W_ = tf.Variable(np.transpose(W), dtype=tf.float32, name='W_{0}'.format(t), trainable = True)\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_,W_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "def build_BALISTA(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    Wmat = sio.loadmat('Wblock.mat') #W has to be precomputed before running the script\n",
        "    W = Wmat.get('W')\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    W_ = tf.Variable(np.transpose(W), dtype=tf.float32, trainable = False)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers, W\n",
        "\n",
        "\n",
        "def build_BALISTA_v2(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "\n",
        "    W = bst.linalg.pinv(A.T)\n",
        "\n",
        "    W_ = tf.Variable(W.T, dtype=tf.float32, trainable = False)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_, gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_, gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers, W\n",
        "\n",
        "def build_BALISTA_v3(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    W = A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "\n",
        "    W = bst.compute_W_v1(A, 32, 16)\n",
        "\n",
        "    pdb.set_trace()\n",
        "\n",
        "    W_ = tf.Variable(W.T, dtype=tf.float32, trainable = False)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_, gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers, W\n",
        "\n",
        "def build_BALISTA_v4(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    # pdb.set_trace()\n",
        "    #W = bst.compute_W_Lagrange(A, 32, 3)\n",
        "\n",
        "    #solve the following problem to get the analytical weight matrix\n",
        "    # D = prob.A\n",
        "    # n = prob.L\n",
        "    # d = prob.B\n",
        "    # m = prob.m\n",
        "    # B_up = cp.Variable((m, n * d))\n",
        "    #\n",
        "    # I = np.kron(np.eye(n), np.ones((d, d)))\n",
        "    # k = np.tile(np.eye(d), (1, n)).T\n",
        "    # b = cp.multiply(B_up.T @ D, I)\n",
        "    # b = cp.matmul(b, k)  # extracting the diagonal blocks of D^TB\n",
        "    # constraints = [b == k]\n",
        "    # objective = cp.Minimize(1 / d * (cp.norm(B_up.T @ D, 'fro')))\n",
        "    # prob = cp.Problem(objective, constraints)\n",
        "    # result = prob.solve()\n",
        "    #\n",
        "    # W = B_up.value\n",
        "    Wmat = sio.loadmat('sparse_case_W_up.mat')\n",
        "    W = Wmat.get('W')\n",
        "    W_ = tf.Variable(W.T, dtype=tf.float32, trainable = False)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(tf.math.abs(gamma0_)*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_, gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(tf.math.abs(gamma_)*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers, W\n",
        "\n",
        "def build_BALISTA_v5(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    # pdb.set_trace()\n",
        "    #W = bst.compute_W_Lagrange(A, 32, 3)\n",
        "    # solve the following problem to get the analytical weight matrix\n",
        "    # D = prob.A\n",
        "    # n = prob.L\n",
        "    # d = prob.B\n",
        "    # m = prob.m\n",
        "    # B_cvx = cp.Variable((m, n * d))\n",
        "    #\n",
        "    # I = np.kron(np.eye(n), np.ones((d, d)))\n",
        "    # k = np.tile(np.eye(d), (1, n)).T\n",
        "    # b = cp.multiply(B_cvx.T @ D, I)\n",
        "    # b = cp.matmul(b, k)  # extracting the diagonal blocks of D^TB\n",
        "    # constraints = [b == k]\n",
        "    # objective = cp.Minimize(1/d*cp.max(R(B_cvx.T@D-np.eye(n*d), n, d)))\n",
        "    # prob = cp.Problem(objective, constraints)\n",
        "    # result = prob.solve()\n",
        "    # W = B_cvx.value\n",
        "    Wmat = sio.loadmat('small_normalized_W_cvx.mat')\n",
        "    W = Wmat.get('W_cvx')\n",
        "    W_ = tf.Variable(W.T, dtype=tf.float32, trainable = False)\n",
        "    #layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(tf.math.abs(gamma0_)*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_, gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(tf.math.abs(gamma_)*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers, W\n",
        "\n",
        "def build_TiLISTA(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = simple_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    #Wmat = sio.loadmat('W.mat') #W has to be precomputed before running the script\n",
        "    W = A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    W_ = tf.Variable(np.transpose(W), dtype=tf.float32, trainable = True)\n",
        "    layers.append(('Linear', tf.matmul(2*gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(2*gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(2*gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "\n",
        "def build_TiBLISTA(prob, T, initial_lambda=.1, initial_gamma=1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    #Wmat = sio.loadmat('W.mat') #W has to be precomputed before running the script\n",
        "    W = A\n",
        "    gamma = initial_gamma\n",
        "    gamma0_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_0')\n",
        "    W_ = tf.Variable(np.transpose(W), dtype=tf.float32, trainable = True)\n",
        "    layers.append(('Linear', tf.matmul(gamma0_*W_, prob.y_), None))\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(tf.matmul(gamma0_*W_, prob.y_), lam0_)\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,gamma0_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        gamma_ = tf.Variable(gamma, dtype=tf.float32, name='gamma_{0}'.format(t))\n",
        "        xhat_ = blocksoft(xhat_ - tf.matmul(gamma_*W_, tf.matmul(prob.A_, xhat_) - prob.y_), lam_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,gamma_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "def build_LBFISTA(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft=block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    By_ = tf.matmul(B_,prob.y_)\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    #pdb.set_trace()\n",
        "    layers.append( ('Linear',By_,None) )\n",
        "    \n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    #if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = blocksoft( By_, lam0_)\n",
        "    tk = (1+np.sqrt(1+4*1**2))*2**(-1)\n",
        "    z_ = xhat_\n",
        "    layers.append( ('LBISTA T=1',xhat_, (lam0_,) ) )\n",
        "    #pdb.set_trace()\n",
        "    for t in range(1,T):\n",
        "        t_prev = tk\n",
        "        xhat_prev_ = xhat_ \n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        xhat_ = blocksoft( tf.matmul(S_,z_) + By_, lam_ )\n",
        "        tk = (1+np.sqrt(1+4*t_prev**2))*2**(-1)\n",
        "        z_ = xhat_ + (t_prev-1)*(tk)**(-1)*(xhat_-xhat_prev_)\n",
        "        layers.append( ('LBISTA T='+str(t+1),xhat_,(lam_,)) )\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "\n",
        "def build_LBFISTA_idea(prob, T, initial_lambda=.1, untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    blocksoft = block_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M, N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A, 2) ** 2)\n",
        "    B_ = tf.Variable(B, dtype=tf.float32, name='B_0')\n",
        "    By_ = tf.matmul(B_, prob.y_)\n",
        "    S_ = tf.Variable(np.identity(N) - np.matmul(B, A), dtype=tf.float32, name='S_0')\n",
        "    # pdb.set_trace()\n",
        "    layers.append(('Linear', By_, None))\n",
        "\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    # if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    lam0_ = tf.Variable(initial_lambda, name='lam_0')\n",
        "    xhat_ = blocksoft(By_, lam0_)\n",
        "    tk_ = tf.Variable((1 + np.sqrt(1 + 4 * 1 ** 2)) * 2 ** (-1), name='t_0')\n",
        "    z_ = xhat_\n",
        "    layers.append(('LBISTA T=1', xhat_, (lam0_,tk_)))\n",
        "    # pdb.set_trace()\n",
        "    for t in range(1, T):\n",
        "        t_prev = tk_.read_value()\n",
        "        t_prev_ = tk_\n",
        "        xhat_prev_ = xhat_\n",
        "        lam_ = tf.Variable(initial_lambda, name='lam_{0}'.format(t))\n",
        "        xhat_ = blocksoft(tf.matmul(S_, z_) + By_, lam_)\n",
        "        tk_ = tf.Variable((1 + tf.sqrt(1 + 4 * t_prev_ ** 2)) * 2 ** (-1), name='t_{0}'.format(t))\n",
        "        z_ = xhat_ + (t_prev_ - 1) * (tk_) ** (-1) * (xhat_ - xhat_prev_)\n",
        "        layers.append(('LBISTA T=' + str(t + 1), xhat_, (lam_,tk_)))\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "def build_LBelastic_net(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    eta=block_soft_threshold_elastic_net\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    By_ = tf.matmul(B_,prob.y_)\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    #pdb.set_trace()\n",
        "    layers.append( ('Linear',By_,None) )\n",
        "    \n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    #if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    al0_ = tf.Variable( initial_lambda,name='al_0')\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = eta( By_, al0_, lam0_)\n",
        "    layers.append( ('LBISTA T=1',xhat_, (al0_, lam0_) ) )\n",
        "    #pdb.set_trace()\n",
        "    for t in range(1,T):\n",
        "        al_ = tf.Variable( initial_lambda,name='al_{0}'.format(t) )\n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        xhat_ = eta( tf.matmul(S_,xhat_) + By_, al_, lam_ )\n",
        "        layers.append( ('LBISTA T='+str(t+1),xhat_,(al_, lam_)) )\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "  \n",
        "def build_UntiedLBelastic_net(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    eta=block_soft_threshold_elastic_net\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    By_ = tf.matmul(B_,prob.y_)\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    #pdb.set_trace()\n",
        "    #layers.append( ('Linear',By_,None) )\n",
        "    \n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    #if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    al0_ = tf.Variable( initial_lambda,name='al_0')\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = eta( By_, al0_, lam0_)\n",
        "    layers.append( ('LBISTA T=1',xhat_, (al0_, lam0_, B_, S_) ) )\n",
        "    #pdb.set_trace()\n",
        "    for t in range(1,T):\n",
        "        al_ = tf.Variable( initial_lambda,name='al_{0}'.format(t) )\n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        B_ =  tf.Variable(B,dtype=tf.float32,name='B_{0}'.format(t) )\n",
        "        By_ = tf.matmul(B_,prob.y_)\n",
        "        S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_{0}'.format(t) )\n",
        "        xhat_ = eta( tf.matmul(S_,xhat_) + By_, al_, lam_ )\n",
        "        layers.append( ('LBISTA T='+str(t+1),xhat_,(al_, lam_, B_, S_)) )\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "def build_LBFastelastic_net(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    prob            - is a TFGenerator which contains problem parameters and def of how to generate training data\n",
        "    initial_lambda  - could be some parameter of Block ISTA <- DELETE if unnecessary\n",
        "    untied          - flag for tied or untied case\n",
        "    Return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    eta=block_soft_threshold_elastic_net\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    By_ = tf.matmul(B_,prob.y_)\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    #pdb.set_trace()\n",
        "    layers.append( ('Linear',By_,None) )\n",
        "    \n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    #if getattr(prob,'iid',True) == False:\n",
        "    #    # create a parameter for each coordinate in x\n",
        "    #    initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    al0_ = tf.Variable( initial_lambda,name='al_0')\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = eta( By_, al0_, lam0_)\n",
        "    tk = (1+np.sqrt(1+4*1**2))*2**(-1)\n",
        "    z_ = xhat_\n",
        "    layers.append( ('LBISTA T=1',xhat_, (al0_, lam0_) ) )\n",
        "    #pdb.set_trace()\n",
        "    for t in range(1,T):\n",
        "        t_prev = tk\n",
        "        xhat_prev_ = xhat_ \n",
        "        al_ = tf.Variable( initial_lambda,name='al_{0}'.format(t) )\n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        xhat_ = eta( tf.matmul(S_,z_) + By_, al_, lam_ )\n",
        "        tk = (1+np.sqrt(1+4*t_prev**2))*2**(-1)\n",
        "        z_ = xhat_ + (t_prev-1)*(tk)**(-1)*(xhat_-xhat_prev_)\n",
        "        layers.append( ('LBISTA T='+str(t+1),xhat_,(al_, lam_)) )\n",
        "\n",
        "    \"\"\"\n",
        "    # check other functions in this file (e.g., build_LISTA and build_LAMP4SSC) to implement LBISTA network\n",
        "    # send me questions if needed\n",
        "    \"\"\"\n",
        "\n",
        "    return layers\n",
        "\n",
        "\n",
        "def build_LISTA(prob,T,initial_lambda=.1,untied=False):\n",
        "    \"\"\"\n",
        "    Builds a LISTA network to infer x from prob.y_ = matmul(prob.A,x) + AWGN\n",
        "    return a list of layer info (name,xhat_,newvars)\n",
        "     name : description, e.g. 'LISTA T=1'\n",
        "     xhat_ : that which approximates x_ at some point in the algorithm\n",
        "     newvars : a tuple of layer-specific trainable variables\n",
        "    \"\"\"\n",
        "    assert not untied,'TODO: untied'\n",
        "    eta = simple_soft_threshold\n",
        "    layers = []\n",
        "    A = prob.A\n",
        "    M,N = A.shape\n",
        "    B = A.T / (1.01 * la.norm(A,2)**2)\n",
        "    B_ =  tf.Variable(B,dtype=tf.float32,name='B_0')\n",
        "    S_ = tf.Variable( np.identity(N) - np.matmul(B,A),dtype=tf.float32,name='S_0')\n",
        "    By_ = tf.matmul( B_ , prob.y_ )\n",
        "    layers.append( ('Linear',By_,None) )\n",
        "\n",
        "    initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
        "    if getattr(prob,'iid',True) == False:\n",
        "        # create a parameter for each coordinate in x\n",
        "        initial_lambda = initial_lambda*np.ones( (N,1),dtype=np.float32 )\n",
        "    lam0_ = tf.Variable( initial_lambda,name='lam_0')\n",
        "    xhat_ = eta( By_, lam0_)\n",
        "    #pdb.set_trace()\n",
        "    layers.append( ('LISTA T=1',xhat_, (lam0_,) ) )\n",
        "    for t in range(1,T):\n",
        "        lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) )\n",
        "        xhat_ = eta( tf.matmul(S_,xhat_) + By_, lam_ )\n",
        "        layers.append( ('LISTA T='+str(t+1),xhat_,(lam_,)) )\n",
        "    return layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhBgZFZHiYBh"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twf_kqvOhUji"
      },
      "source": [
        "def load_trainable_vars(sess, filename):\n",
        "    \"\"\"load a .npz archive and assign the value of each loaded\n",
        "    ndarray to the trainable variable whose name matches the\n",
        "    archive key.  Any elements in the archive that do not have\n",
        "    a corresponding trainable variable will be returned in a dict.\n",
        "    \"\"\"\n",
        "    other = {}\n",
        "    try:\n",
        "        tv = dict([(str(v.name), v) for v in tf.trainable_variables()])\n",
        "        for k, d in np.load(filename).items():\n",
        "            if k in tv:\n",
        "                print('restoring ' + k + ' is:' + str(d))\n",
        "                sess.run(tf.assign(tv[k], d))\n",
        "            else:\n",
        "                other[k] = d\n",
        "    except IOError:\n",
        "        pass\n",
        "    return other\n",
        "\n",
        "\n",
        "def save_trainable_vars(sess, filename, **kwargs):\n",
        "    \"\"\"save a .npz archive in `filename`  with\n",
        "    the current value of each variable in tf.trainable_variables()\n",
        "    plus any keyword numpy arrays.\n",
        "    \"\"\"\n",
        "    save = {}\n",
        "    for v in tf.trainable_variables():\n",
        "        save[str(v.name)] = sess.run(v)\n",
        "    save.update(kwargs)\n",
        "    np.savez(filename, **save)\n",
        "\n",
        "\n",
        "def setup_training(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('WblockLag.mat')\n",
        "    W = Wmat.get('W')\n",
        "    t = 0\n",
        "    mut_block = bst.mutual_block_coherence(W, prob.A, 32, 16)\n",
        "    s = 2.0\n",
        "    d = 8.0\n",
        "    upper_bound = 2/(d*(2*mut_block*s-mut_block)+1)\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        gam_constraints = 0\n",
        "        # pdb.set_trace()\n",
        "        for i in range(0, len(tf.trainable_variables())):\n",
        "            if tf.trainable_variables()[i].name.startswith('gam') and i==t:\n",
        "                gam_constraints = gam_constraints + 100*tf.maximum(tf.trainable_variables()[i], upper_bound)\n",
        "        t = t+1\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_) + gam_constraints\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None:\n",
        "            train_ = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list)\n",
        "            training_stages.append((name, xhat_, loss_, nmse_, se_, train_, var_list))\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "\n",
        "\n",
        "def setup_training_loss2(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('WLag.mat')\n",
        "    W = Wmat.get('W')\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        B = prob.B\n",
        "        L = prob.L\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([L, B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        upper_bound = tf.reduce_max(r_)\n",
        "        gam = 0\n",
        "        lam = 0\n",
        "        for i in range(0, len(var_list)):\n",
        "            if var_list[i].name.startswith('gamma_'+str(t)):\n",
        "                gam = var_list[i]\n",
        "            if var_list[i].name.startswith('lam_'+str(t)):\n",
        "                lam = var_list[i]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints + 10*tf.maximum(lam/gam, upper_bound) - 10*tf.minimum(gam/1, 0)\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_) + constraints\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None:\n",
        "            train_ = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list)\n",
        "            training_stages.append((name, xhat_, loss_, nmse_, se_, train_, var_list))\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "\n",
        "def setup_training_loss3(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('WLag.mat')\n",
        "    W = Wmat.get('W')\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.L, prob.B)\n",
        "        B = prob.B\n",
        "        L = prob.L\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        sigma = np.linalg.norm(prob.noise, 2)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([L, B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        lower_bound = B*mut_block*tf.reduce_max(r_)+C_w*sigma\n",
        "        gam = 0\n",
        "        lam = 0\n",
        "        for i in range(0, len(var_list)):\n",
        "            if var_list[i].name.startswith('gamma_'+str(t)):\n",
        "                gam = var_list[i]\n",
        "            if var_list[i].name.startswith('lam_'+str(t)):\n",
        "                lam = var_list[i]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints - 2*tf.minimum(lam/1, (gam/1)*(lower_bound)) - 2*tf.minimum(gam/1, 0)\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_) + constraints\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None:\n",
        "            train_ = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list)\n",
        "            training_stages.append((name, xhat_, loss_, nmse_, se_, train_, var_list))\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "\n",
        "def setup_training_loss4(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('WLag.mat')\n",
        "    W = Wmat.get('W')\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.L, prob.B)\n",
        "        B = prob.B\n",
        "        L = prob.L\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        sigma = np.linalg.norm(prob.noise, 2)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([L, B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        lower_bound = B*mut_block*tf.reduce_max(r_)+C_w*sigma\n",
        "        gam = 0\n",
        "        lam = 0\n",
        "        for i in range(0, len(var_list)):\n",
        "            if var_list[i].name.startswith('gamma_'+str(t)):\n",
        "                gam = var_list[i]\n",
        "            if var_list[i].name.startswith('lam_'+str(t)):\n",
        "                lam = var_list[i]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints - 2*tf.minimum(lam/1, (gam/1)*(lower_bound)) - 2*tf.minimum(gam/1, 0)\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_) + constraints\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None: #IDEA train at first only gamma, after this learn lambdas...\n",
        "            train_gam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[1])\n",
        "            training_stages.append((name+'only Gamma', xhat_, loss_, nmse_, se_, train_gam, [var_list[1]]))\n",
        "            train_lam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[0])\n",
        "            training_stages.append((name+'only Lambda', xhat_, loss_, nmse_, se_, train_lam, [var_list[0]]))\n",
        "\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "\n",
        "def setup_training_loss_max(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), omega=5, final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('WLag.mat')\n",
        "    W = Wmat.get('W')\n",
        "    sigma = np.ceil(np.mean(np.linalg.norm(prob.noise, 2, axis=0)))\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        # pdb.set_trace()\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.m, prob.L, prob.B)\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([prob.L, prob.B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        r_ = tf.reduce_sum(r_,0)\n",
        "        lower_bound = prob.B*mut_block*tf.reduce_max(r_)+C_w*sigma\n",
        "        # gam = 0\n",
        "        # lam = 0\n",
        "        for i in range(0, len(var_list)):\n",
        "            if var_list[i].name.startswith('gamma_'+str(t)):\n",
        "                gam = var_list[i]\n",
        "            if var_list[i].name.startswith('lam_'+str(t)):\n",
        "                lam = var_list[i]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_)\n",
        "        #loss_2 = tf.nn.l2_loss(xhat_ - prob.x_) - omega*tf.minimum(lam/1, (tf.math.abs(gam)/1)*(lower_bound))\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None: #IDEA train at first only gamma, after this learn lambdas...\n",
        "            train_gam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[1])\n",
        "            training_stages.append((name+' only Gamma', xhat_, loss_, nmse_, se_, train_gam, [var_list[1]]))\n",
        "            loss_2 = loss_ - omega*tf.minimum(var_list[0]/1 , (tf.math.abs(var_list[1])/1)*(lower_bound))\n",
        "            train_lam = tf.train.AdamOptimizer(tr_).minimize(loss_2, var_list=var_list[0])\n",
        "            training_stages.append((name+' only Lambda', xhat_, loss_, nmse_, se_, train_lam, [var_list[0]])) #welches loss mur nach xhat stehen?\n",
        "\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "def setup_training_loss_mean(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), omega = 5, final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('WLag.mat')\n",
        "    W = Wmat.get('W')\n",
        "    sigma = np.ceil(np.mean(np.linalg.norm(prob.noise, 2, axis=0)))\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        # pdb.set_trace()\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.m, prob.L, prob.B)\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([prob.L, prob.B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        r_ = tf.reduce_sum(r_,0)\n",
        "        lower_bound = prob.B*mut_block*tf.reduce_mean(r_)+C_w*sigma\n",
        "        # gam = 0\n",
        "        # lam = 0\n",
        "        for i in range(0, len(var_list)):\n",
        "            if var_list[i].name.startswith('gamma_'+str(t)):\n",
        "                gam = var_list[i]\n",
        "            if var_list[i].name.startswith('lam_'+str(t)):\n",
        "                lam = var_list[i]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_)\n",
        "        #loss_2 = tf.nn.l2_loss(xhat_ - prob.x_) - omega*tf.minimum(lam/1, (tf.math.abs(gam)/1)*(lower_bound))\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None: #IDEA train at first only gamma, after this learn lambdas...\n",
        "            train_gam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[1])\n",
        "            training_stages.append((name+' only Gamma', xhat_, loss_, nmse_, se_, train_gam, [var_list[1]]))\n",
        "            loss_2 = loss_ - omega*tf.minimum([var_list[0]/1 , (tf.math.abs(var_list[1])/1)*(lower_bound)])\n",
        "            train_lam = tf.train.AdamOptimizer(tr_).minimize(loss_2, var_list=var_list[0])\n",
        "            training_stages.append((name+' only Lambda', xhat_, loss_, nmse_, se_, train_lam, [var_list[0]])) #welches loss mur nach xhat stehen?\n",
        "\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "def setup_training_loss_mean_LSE(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01),omega=5, final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('W_cvx.mat')\n",
        "    W = Wmat.get('W_cvx')\n",
        "    sigma = np.ceil(np.mean(np.linalg.norm(prob.noise, 2, axis=0)))\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        # pdb.set_trace()\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.m, prob.L, prob.B)\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([prob.L, prob.B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        r_ = tf.reduce_sum(r_,0)\n",
        "        lower_bound = prob.B*mut_block*tf.reduce_mean(r_)+C_w*sigma\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_)\n",
        "        #loss_2 = tf.nn.l2_loss(xhat_ - prob.x_) + gam*tf.math.reduce_logsumexp([ - lam/1 , - (tf.math.abs(gam)/1)*(lower_bound)])\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None: #IDEA train at first only gamma, after this learn lambdas...\n",
        "            train_gam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[1])\n",
        "            training_stages.append((name+' only Gamma', xhat_, loss_, nmse_, se_, train_gam, [var_list[1]]))\n",
        "            loss_2 = loss_ + omega*tf.math.reduce_logsumexp([ - var_list[0]/1 , - (tf.math.abs(var_list[1])/1)*(lower_bound)])\n",
        "            train_lam = tf.train.AdamOptimizer(tr_).minimize(loss_2, var_list=var_list[0])\n",
        "            training_stages.append((name+' only Lambda', xhat_, loss_, nmse_, se_, train_lam, [var_list[0]])) #welches loss mur nach xhat stehen?\n",
        "\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "\n",
        "def setup_training_loss_max_LSE(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01),omega=5, final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('small_normalized_W_cvx.mat')\n",
        "    W = Wmat.get('W_cvx')\n",
        "    sigma = np.ceil(np.mean(np.linalg.norm(prob.noise, 2, axis=0)))\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        # pdb.set_trace()\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.m, prob.L, prob.B)\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([prob.L, prob.B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        r_ = tf.reduce_sum(r_,0)\n",
        "        lower_bound = prob.B*mut_block*tf.reduce_max(r_)+C_w*sigma\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_)\n",
        "        #loss_2 = tf.nn.l2_loss(xhat_ - prob.x_) + gam*tf.math.reduce_logsumexp([ - lam/1 , - (tf.math.abs(gam)/1)*(lower_bound)])\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None: #IDEA train at first only gamma, after this learn lambdas...\n",
        "            train_gam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[1])\n",
        "            training_stages.append((name+' only Gamma', xhat_, loss_, nmse_, se_, train_gam, [var_list[1]]))\n",
        "            loss_2 = loss_ + omega*tf.math.reduce_logsumexp([ - var_list[0]/1 , - (tf.math.abs(var_list[1])/1)*(lower_bound)])\n",
        "            train_lam = tf.train.AdamOptimizer(tr_).minimize(loss_2, var_list=var_list[0])\n",
        "            training_stages.append((name+' only Lambda', xhat_, loss_2, nmse_, se_, train_lam, [var_list[0]])) #welches loss mur nach xhat stehen?\n",
        "\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "def setup_training_loss_mean_LSE_learn_omega(layer_info, prob, trinit=1e-3, refinements=(.5, .1, .01), final_refine=None):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\" Given a list of layer info (name,xhat_,newvars),\n",
        "    create an output list of training operations (name,xhat_,loss_,nmse_,trainop_ ).\n",
        "    Each layer_info element will be split into one or more output training operations\n",
        "    based on the presence of newvars and len(refinements)\n",
        "    \"\"\"\n",
        "    losses_ = []\n",
        "    nmse_ = []\n",
        "    trainers_ = []\n",
        "    assert np.array(refinements).min() > 0, 'all refinements must be in (0,1]'\n",
        "    assert np.array(refinements).max() <= 1, 'all refinements must be in (0,1]'\n",
        "\n",
        "    maskX_ = getattr(prob, 'maskX_', 1)\n",
        "    if maskX_ != 1:\n",
        "        print('masking out inconsequential parts of signal x for nmse reporting')\n",
        "\n",
        "    nmse_denom_ = tf.nn.l2_loss(prob.x_ * maskX_)\n",
        "\n",
        "    tr_ = tf.Variable(trinit, name='tr', trainable=False)\n",
        "    training_stages = []\n",
        "    Wmat = sio.loadmat('small_normalized_W_cvx.mat')\n",
        "    W = Wmat.get('W_cvx')\n",
        "    sigma = np.ceil(np.mean(np.linalg.norm(prob.noise, 2, axis=0)))\n",
        "    t = 0\n",
        "    for name, xhat_, var_list in layer_info:\n",
        "        constraints = 0\n",
        "        # pdb.set_trace()\n",
        "        mut_block = bst.mutual_block_coherence(W, prob.A, prob.m, prob.L, prob.B)\n",
        "        C_w = bst.C_w(W, prob.L, prob.B)\n",
        "        shape = K.shape(prob.x_)\n",
        "        pool_shape1 = tf.stack([prob.L, prob.B, shape[1]])\n",
        "        x_reshaped = K.reshape(prob.x_, pool_shape1)\n",
        "        x_hat_reshaped = K.reshape(xhat_, pool_shape1)\n",
        "        r_ = tf.sqrt(tf.reduce_sum(((x_reshaped-x_hat_reshaped) ** 2), 1))\n",
        "        r_ = tf.reduce_sum(r_,0)\n",
        "        lower_bound = prob.B*mut_block*tf.reduce_mean(r_)+C_w*sigma\n",
        "        # gam = 0\n",
        "        # lam = 0\n",
        "        for i in range(0, len(var_list)):\n",
        "            if var_list[i].name.startswith('gamma_'+str(t)):\n",
        "                gam = var_list[i]\n",
        "            if var_list[i].name.startswith('lam_'+str(t)):\n",
        "                lam = var_list[i]\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        constraints = constraints\n",
        "        omega_ = tf.Variable(5, dtype=tf.float32, name='omega_')\n",
        "        loss_ = tf.nn.l2_loss(xhat_ - prob.x_)\n",
        "        loss_2 = tf.nn.l2_loss(xhat_ - prob.x_) + omega_*tf.math.reduce_logsumexp([ - lam/1 , - (tf.math.abs(gam)/1)*(lower_bound)])\n",
        "        nmse_ = tf.nn.l2_loss((xhat_ - prob.x_) * maskX_) / nmse_denom_\n",
        "        t = t+1\n",
        "        \"sigma2_ = tf.reduce_mean(rvar_)\"\n",
        "        \"sigma2_empirical_ = tf.reduce_mean((rhat_ - prob.x_)**2)\"\n",
        "        #pdb.set_trace()\n",
        "        se_ = 2 * tf.nn.l2_loss(xhat_ - prob.x_)  # to get MSE, divide by / (L * N)\n",
        "\n",
        "        #if name == 'LBISTA T=6':\n",
        "        if var_list is not None: #IDEA train at first only gamma, after this learn lambdas...\n",
        "            train_gam = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list[1])\n",
        "            training_stages.append((name+' only Gamma', xhat_, loss_, nmse_, se_, train_gam, [var_list[1]]))\n",
        "            train_lam = tf.train.AdamOptimizer(tr_).minimize(loss_2, var_list=[var_list[0],omega_])\n",
        "            training_stages.append((name+' only Lambda', xhat_, loss_, nmse_, se_, train_lam, [var_list[0],omega_])) #welches loss mur nach xhat stehen?\n",
        "\n",
        "        # for fm in refinements:\n",
        "        #      train2_ = tf.train.AdamOptimizer(tr_ * fm).minimize(loss_)\n",
        "        #      training_stages.append((name + ' trainrate=' + str(fm), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "    if final_refine:\n",
        "        train2_ = tf.train.AdamOptimizer(tr_ * final_refine).minimize(loss_)\n",
        "        training_stages.append((name + ' final refine ' + str(final_refine), xhat_, loss_, nmse_, se_, train2_, ()))\n",
        "\n",
        "    return training_stages\n",
        "\n",
        "\n",
        "\n",
        "def do_training(training_stages, prob, savefile, ivl=10, maxit=100000, better_wait=5000):\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    \"\"\"\n",
        "    ivl:how often should we compute the nmse of the validation set?\n",
        "    maxit: max number of training iterations\n",
        "    better_wait:wait this many iterations for an nmse that is better than the prevoius best of the current training session\n",
        "    \"\"\"\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    print('norms xval:{xval:.7f} yval:{yval:.7f}'.format(xval=la.norm(prob.xval), yval=la.norm(prob.yval)))\n",
        "\n",
        "    state = load_trainable_vars(sess, savefile)  # must load AFTER the initializer\n",
        "\n",
        "    # must use this same Session to perform all training\n",
        "    # if we start a new Session, things would replay and we'd be training with our validation set (no no)\n",
        "\n",
        "    done = state.get('done', [])\n",
        "    log = str(state.get('log', ''))\n",
        "\n",
        "    for name, xhat_, loss_, nmse_, se_, train_, var_list in training_stages:\n",
        "        start = time.time()\n",
        "        if name in done:\n",
        "            print('Already did ' + name + '. Skipping.')\n",
        "            continue\n",
        "        if len(var_list):\n",
        "            describe_var_list = 'extending ' + ','.join([v.name for v in var_list])\n",
        "        else:\n",
        "            describe_var_list = 'fine tuning all ' + ','.join([v.name for v in tf.trainable_variables()])\n",
        "\n",
        "        print(name + ' ' + describe_var_list)\n",
        "        nmse_history = []\n",
        "\n",
        "        for i in range(maxit + 1):\n",
        "\n",
        "            if i % ivl == 0:\n",
        "                nmse = sess.run(nmse_, feed_dict={prob.y_: prob.yval, prob.x_: prob.xval})\n",
        "                if np.isnan(nmse):\n",
        "                    #pdb.set_trace()\n",
        "                    raise RuntimeError('nmse is NaN')\n",
        "                nmse_history = np.append(nmse_history, nmse)\n",
        "                nmse_dB = 10 * np.log10(nmse)\n",
        "                nmsebest_dB = 10 * np.log10(nmse_history.min())\n",
        "                sys.stdout.write(\n",
        "                    '\\ri={i:<6d} nmse={nmse:.6f} dB (best={best:.6f})'.format(i=i, nmse=nmse_dB, best=nmsebest_dB))\n",
        "                sys.stdout.flush()\n",
        "                if i % (100 * ivl) == 0:\n",
        "                    print('')\n",
        "                    age_of_best = len(nmse_history) - nmse_history.argmin() - 1  # how long ago was the best nmse?\n",
        "                    if age_of_best * ivl > better_wait:\n",
        "                        break  # if it has not improved on the best answer for quite some time, then move along\n",
        "            y, x = prob(sess)\n",
        "            sess.run(train_, feed_dict={prob.y_: y, prob.x_: x})  # hier fehler lam0=nan\n",
        "\n",
        "        done = np.append(done, name)\n",
        "\n",
        "        end = time.time()\n",
        "        time_log = 'Took me {totaltime:.3f} minutes, or {time_per_interation:.1f} ms per iteration'.format(\n",
        "            totaltime=(end - start) / 60, time_per_interation=(end - start) * 1000 / i)\n",
        "        print(time_log)\n",
        "        log = log + '\\n{name} nmse={nmse:.6f} dB in {i} iterations'.format(name=name, nmse=nmse_dB, i=i)\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        state['done'] = done\n",
        "        state['log'] = log\n",
        "        save_trainable_vars(sess, savefile, **state)\n",
        "    return sess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPVFnSpviaDx"
      },
      "source": [
        "# Main Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi4aU4iahav4"
      },
      "source": [
        "prob = block_gaussian_trial(m=50, L=15, B=5, MC=250, pnz=.1, SNR_dB=-10) # a Block-Gaussian x, noisily observed through a random matrix\n",
        "T=16\n",
        "layers,W = build_BALISTA_v5(prob,T=T,initial_lambda=.1,untied=False)\n",
        "#layers = build_LBFISTA(prob,T=6,initial_lambda=.1,untied=False)\n",
        "#layers = build_LBelastic_net(prob,T=6,initial_lambda=.1,untied=False)\n",
        "#layers = build_LBFastelastic_net(prob,T=6,initial_lambda=.1,untied=False)\n",
        "#layers = build_UntiedLBelastic_net(prob,T=6,initial_lambda=.1,untied=False)\n",
        "start = time.time()\n",
        "training_stages = setup_training_loss_max_LSE(layers,prob,trinit=1e-3,refinements=(.5,0.1,.01))\n",
        "end = time.time()\n",
        "print( 'Took me {totaltime:.3f} minutes for setup training'.format(totaltime = (end-start)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTGiHFCUhjKW"
      },
      "source": [
        "sess = do_training(training_stages,prob,'LBISTA_block_Gauss_giidT16Thermo6-JanWithGamConstraints-10.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUoydfC8iecw"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RL1Tz25h6ky"
      },
      "source": [
        "sparsemat = sio.loadmat('D_blocktestSNR' + str(-10) + '.mat')\n",
        "y = sparsemat.get('y')\n",
        "x = sparsemat.get('x')\n",
        "MC = 250\n",
        "t=0\n",
        "l2norm=np.zeros(((T),MC))\n",
        "nmse=np.zeros(((T),MC))\n",
        "for name, xhat_, var_list in layers:\n",
        "    if not name=='Linear':\n",
        "        xhat = sess.run(xhat_, feed_dict={prob.y_: y, prob.x_: x})\n",
        "        for i in range(0, x.shape[1]):\n",
        "            nmse[t,i]=bst.nmse(xhat[:,i, np.newaxis], x[:,i, np.newaxis])\n",
        "            l2norm[t, i] = bst.l21norm(xhat[:, i]- x[:, i], prob.L, prob.B)\n",
        "        t+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmORePTzHdkc"
      },
      "source": [
        "lam = np.zeros(T)\n",
        "gam = np.zeros(T)\n",
        "for i in range(0,T):\n",
        "  lam[i]=sess.run(layers[i][2][0])\n",
        "  gam[i]=sess.run(layers[i][2][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5YccxGWMnNL"
      },
      "source": [
        "plt.plot(lam/gam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNHTDUrqURng"
      },
      "source": [
        "plt.plot(lam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G3U_45QUSd4"
      },
      "source": [
        "plt.plot(gam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk58x5OnlREe"
      },
      "source": [
        "plt.plot(10*np.log10(np.mean(np.ma.masked_invalid(nmse), axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okAU7Tg2Wf_f"
      },
      "source": [
        "plt.plot(xhat[:,99])\n",
        "plt.plot(x[:,99])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysDOMs47WkAb"
      },
      "source": [
        "plt.plot(x[:,99])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}